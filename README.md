# SyncMain

To overcome communication barriers for the deaf and mute community, we propose GestureTalkâ€”a Human-Computer Interface (HCI) that interprets Sign Language using computer vision and machine learning. This real-time system translates gestures into text and speech with high accuracy, supports multiple languages (Hindi, gujarati, Spanish, French, etc.). 

1. Computer-based recognition: Accurately detects hand gestures and complex using computer vision.
2. Facial expression detection: Captures emotions to add clarity to communication.
3. High-accuracy ML model: Random Forest-based model with adaptive learning for improved gesture detection.
4. Speech-to-sign conversion: Converts spoken language into corresponding sign language representations.
5. Video calling with live sign translation: Video calls with real-time gesture-to-text translation for hearing users.
6. AI chatbot assistant: Guides users, answers questions, and helps troubleshoot.
7. Real-time gesture translation: Instantly converts gestures to text and speech; supports multilingual output.
8. User-friendly interface: Simple UI with clear feedback for deaf and hearing users.
9. Smart assistance: Backspace for corrections; intelligent word suggestions.
10 Cross-platform support: Works on smartphones, tablets, and computers; supports multiple sign and spoken languages.


Installation
Clone the repo:

git clone https://github.com/Sayanika19/SyncMain
cd SyncMain


Create a virtual environment:

python3 -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows


Install dependencies:

pip install -r requirements.txt



Created by Sayanika & Suraj.
